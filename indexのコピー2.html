<html>
<head>
<title>FaceVRM</title>
<script src="https://cdn.jsdelivr.net/npm/three@0.106.2/build/three.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/scatter-gl@0.0.1/lib/scatter-gl.min.js"/>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"></script>
<script src="https://aframe.io/releases/1.0.4/aframe.min.js"></script>

<style>
body {
  margin: 0;
  padding: 0;
  background-color: black;
}

.output_screean { 
  position: relative;
  z-index: 1;
}
.vrmCanvas { 
  position: absolute;
  z-index: 2;
  background: gold;
  width: 400px;
  height: 350px;
}

.canvas-wrapper { 
  position: absolute;
  z-index: 4;
  width: 98%;
  left: 10px;
  top: 370px;
}

.input_text {
  left: 50px;
  top: 30px;
  width: 300px;
  height: 300px;
  position: absolute;
  z-index: 5;
  font-size: 30px;
  background: transparent;
  border-style: none;
  color: white;
  opacity: 0.9;
}
</style>
</head>
<body>
<div id="main">
  <div class="container">
    <div class="output_screean">
      <div id="vrmCanvas" class="vrmCanvas"></div>
      <div class="textarea"></div> 
      <div><textarea class="input_text"></textarea></div>
    </div>
    <div class="canvas-wrapper">
      <canvas id="output"></canvas>
      <video id="video" playsinline style="
        -webkit-transform: scaleX(-1);
        transform: scaleX(-1);
        visibility: hidden;
        width: auto;
        height: auto;
        ">
      </video>
    </div>
  </div>
</div>
  <a-scene background="color: #FAFAFA" stats>

   <a-camera look-controls-enabled="false" wasd-controls-enabled="false" position="0 0 0"></a-camera>
   <a-assets>
    <img id="img" src="./cxm.png" style="
   -webkit-transform:scaleX(-1);
   transform:scaleX(-1);
   visibility:hidden;
   width:300px;height:300px;">
   </a-assets>
   <a-plane position="0 1 -4" rotation="0 0 0" scale="-1 1 1" width="4" height="4" src="#video" ></a-plane>
   <a-entity id="face" position="1.5 3 -3.5" rotation="0 0 0" scale="-0.01 -0.01 -0.01"></a-entity>

  </a-scene>
</body>

<script>
const state = {
  maxFaces: 1,
  triangulateMesh: true
};

// renderer
let vrmCanvas = document.getElementById('vrmCanvas');
const renderer = new THREE.WebGLRenderer();
renderer.setSize( vrmCanvas.clientWidth, vrmCanvas.clientHeight );
renderer.setPixelRatio( window.devicePixelRatio );
vrmCanvas.appendChild(renderer.domElement);

// camera
const camera = new THREE.PerspectiveCamera( 30.0, window.innerWidth / window.innerHeight, 0.1, 20.0 );
camera.position.set( 0.0, 0.6, 1.5 );
const scene = new THREE.Scene();

const light = new THREE.DirectionalLight( 0xffffff );
light.position.set( 1.0, 1.0, 1.0 ).normalize();
scene.add( light );

let model, ctx, videoWidth, videoHeight, video, canvas;

videoWidth = 300;
videoHeight = 250;

let currentVRM = undefined;
const loader = new THREE.GLTFLoader();
loader.load(

    // URL of the VRM you want to load
    '../models/neko_WildBoarSample.vrm',

    // called when the resource is loaded
    ( gltf ) => {

        // generate a VRM instance from gltf
        THREE.VRM.from( gltf ).then( ( vrm ) => {

            // add the loaded vrm to the scene
            scene.add( vrm.scene );
            currentVRM = vrm;

            const hips = vrm.humanoid.getBoneNode( THREE.VRMSchema.HumanoidBoneName.Hips );
            hips.rotation.y = Math.PI;

            vrm.humanoid.setPose( {
              [ THREE.VRMSchema.HumanoidBoneName.LeftUpperArm ]: {
                rotation: new THREE.Quaternion().setFromEuler( new THREE.Euler( 0.0, 0.0, 1.1 ) ).toArray()
              },
              [ THREE.VRMSchema.HumanoidBoneName.RightUpperArm ]: {
                rotation: new THREE.Quaternion().setFromEuler( new THREE.Euler( 0.0, 0.0, -1.1 ) ).toArray()
              },
            } );

            renderer.render( scene, camera );
        } );
    },
    // called while loading is progressing
    ( progress ) => console.log( 'Loading model...', 100.0 * ( progress.loaded / progress.total ), '%' ),

    // called when loading has errors
    ( error ) => console.error( error )
);

main();

async function main() {
  
  await setupCamera();

  video.play();
  videoWidth = video.videoWidth;
  videoHeight = video.videoHeight;
  video.width = videoWidth;
  video.height = videoHeight;

  canvas = document.getElementById('output');
  canvas.width = videoWidth;
  canvas.height = videoHeight;
  const canvasContainer = document.querySelector('.canvas-wrapper');
  canvasContainer.style = `width: ${videoWidth}px; height: ${videoHeight}px`;

  ctx = canvas.getContext('2d');
  ctx.translate(canvas.width, 0);
  ctx.scale(-1, 1);
  ctx.fillStyle = '#32EEDB';
  ctx.strokeStyle = '#32EEDB';
  ctx.lineWidth = 0.5;

  model = await facemesh.load({maxFaces: state.maxFaces});

  renderPrediction();  
}

function head_pose_estimation(faces,rightEye,leftEye)
{
  let fecePoints = tf.tensor(faces);
  let eye1 = tf.tensor1d(rightEye);
  let eye2 = tf.tensor1d(leftEye);

  let scales = fecePoints.div(tf.norm(eye1.sub(eye2))).mul(0.06);
  let centered = scales.sub(scales.mean(axis=0));

  let c00 = centered.slice(0,1).as1D();
  let c09 = centered.slice(9,1).as1D();
  let c18 = centered.slice(18,1).as1D();
  let c27 = centered.slice(27,1).as1D();

  let rotate0 = c18.sub(c00).div(tf.norm(c18.sub(c00)));
  let rotate1 = c09.sub(c27).div(tf.norm(c09.sub(c27)));

  let rotate = tf.concat([rotate0, rotate1]).arraySync();

  // rotation_matrix
  let m00 = rotate[0];
  let m01 = rotate[1];
  let m02 = rotate[2];

  let m10 = rotate[3];
  let m11 = rotate[4];
  let m12 = rotate[5];

  // cross product
  let m20 = m01*m12 - m02*m11;
  let m21 = m02*m10 - m00*m12;
  let m22 = m00*m11 - m01*m10;

  let sy = Math.sqrt(m00 * m00 +  m10 * m10);
  let singular = sy < 10**-6;

  if (!singular){
    yaw = Math.atan2(m21 , m22);
    pitch = Math.atan2(-m20, sy);
    roll = Math.atan2(m10, m00);
  } else {
    yaw = Math.atan2(-m12, m11);
    pitch = Math.atan2(-m20, sy);
    roll = 0;
  }

  const head = currentVRM.humanoid.getBoneNode( THREE.VRMSchema.HumanoidBoneName.Head );
  head.rotation.set( yaw + Math.PI , pitch, -(roll - Math.PI / 2)  , 'XYZ' );
  renderer.render( scene, camera );
  
}

async function renderPrediction() {

  const predictions = await model.estimateFaces(video);
  ctx.drawImage(
      video, 0, 0, videoWidth, videoHeight, 0, 0, canvas.width, canvas.height);

  if (predictions.length > 0) {
    predictions.forEach(prediction => 
   {
      const keypoints = prediction.scaledMesh;
      const annotations = prediction.annotations;

      let rightEyeLower1 = annotations.rightEyeLower1[8];
      let leftEyeLower1 = annotations.leftEyeLower1[8];

          let sp = document.createElement("a-plane");
          let keypointX = keypoints[8][0];
          let keypointY = keypoints[8][1];
          let keypointZ = keypoints[8][2] * 2.5;
          sp.setAttribute("position", `${keypointX} ${keypointY} ${keypointZ}`);
          sp.className = "sp";
          //sp.setAttribute("color", "red");
          sp.setAttribute("src", "#img");
          sp.setAttribute("scale", "-1 1 1");
          sp.setAttribute("rotation", "0 180 180");
          sp.setAttribute("width", "100");
          sp.setAttribute("height", "100");
          faceEntity.appendChild(sp);
      
   
        for (let i = 0; i < TRIANGULATION.length / 3; i++) {
          const points = [
            TRIANGULATION[i * 3], TRIANGULATION[i * 3 + 1],
            TRIANGULATION[i * 3 + 2]
          ].map(index => keypoints[index]);
          drawPath(ctx, points, true);
        }
      
      
      // face dot drawing
      let model_points = [rightEyeLower1,leftEyeLower1];
      for(let i = 0; i < model_points.length; i++) {
        const x = model_points[i][0];
        const y = model_points[i][1];
        ctx.beginPath();
        ctx.fillStyle = "#FF0000";
        ctx.arc(x, y, 3 /* radius */, 0, 2 * Math.PI);
        ctx.fill();
      }

      let contour_points = [0,18,9,27];
      for(let i of contour_points) {
        const x = annotations.silhouette[i][0];
        const y = annotations.silhouette[i][1];
        ctx.beginPath();
        ctx.fillStyle = (i == 0 || i == 18) ? "#FFFF00" : "#0000FF";
        ctx.arc(x, y, 3 /* radius */, 0, 2 * Math.PI);
        ctx.fill();
      }
      head_pose_estimation(annotations.silhouette,rightEyeLower1,leftEyeLower1);
    });
  }
  requestAnimationFrame(renderPrediction);
}

function drawPath(ctx, points, closePath) {
  const region = new Path2D();
  region.moveTo(points[0][0], points[0][1]);
  for (let i = 1; i < points.length; i++) {
    const point = points[i];
    region.lineTo(point[0], point[1]);
  }

  if (closePath) {
    region.closePath();
  }
  ctx.stroke(region);
}

async function setupCamera() {
  video = document.getElementById('video');

  const stream = await navigator.mediaDevices.getUserMedia({
    'audio': false,
    'video': {
      facingMode: 'user',
      // Only setting the video to a specified size in order to accommodate a
      // point cloud, so on mobile devices accept the default size.
      width: videoWidth,
      height: videoHeight
    },
  });
  video.srcObject = stream;

  return new Promise((resolve) => {
    video.onloadedmetadata = () => {
      resolve(video);
    };
  });
}
</script>

</html>








<script>

let scatterGLHasInitialized = false, scatterGL;
let ctx,videoWidth, videoHeight, video, canvas,canvas1;

const stats = new Stats();
var color=[255, 70, 0, 1]; 


function isMobile()
{
  const isAndroid = /Android/i.test(navigator.userAgent);
  const isiOS = /iPhone|iPad|iPod/i.test(navigator.userAgent);
  return isAndroid || isiOS;
}

const VIDEO_SIZE = 300;
const mobile = isMobile();


/*
function drawPath(ctx, points, closePath) 
{
  const region = new Path2D();
  region.moveTo(points[0][0], points[0][1]);
  for (let i = 1; i < points.length; i++) 
  {
    const point = points[i];
    region.lineTo(point[0], point[1]);
  }
  if (closePath) {region.closePath();}
  //ctx.stroke(region);
  ctx.fill(region);
}
*/


      window.onload = function()
      {
        navigator.mediaDevices = navigator.mediaDevices || ((navigator.mozGetUserMedia || navigator.webkitGetUserMedia) ?
        {
           getUserMedia: function(c) {
             return new Promise(function(y, n) {
               (navigator.mozGetUserMedia ||
                navigator.webkitGetUserMedia).call(navigator, c, y, n);
             });
           }
        } : null);

        if (!navigator.mediaDevices)
        {
          console.log("getUserMedia() not supported.");
          return;
        }
        var constraints = { audio: false, video: {
      facingMode: 'user',
      // Only setting the video to a specified size in order to accommodate a
      // point cloud, so on mobile devices accept the default size.
      width: mobile ? undefined : VIDEO_SIZE,
      height: mobile ? undefined : VIDEO_SIZE} };

        navigator.mediaDevices.getUserMedia(constraints)
        .then(function(stream)
  {
          var video = document.querySelector('video');
          video.srcObject = stream
          video.onloadedmetadata = function(e) {
            video.play();

　//videWidth,videHeight:再生している動画サイズ
  videoWidth = video.videoWidth;
  videoHeight = video.videoHeight;
  video.width = videoWidth;
  video.height = videoHeight;

/*
  canvas1 = document.getElementById('output1');
  canvas1.width = videoWidth;
  canvas1.height = videoHeight;
*/

/*
  ctx1 = canvas1.getContext('2d');
  ctx1.translate(canvas.width, 0);
  ctx1.scale(-1, 1);



        ctx1.fillStyle = "rgba(" + color + ")";
        //ctx1.strokeStyle = "rgba(" + [255, 100, 70, 0.15] + ")";
        ctx1.lineWidth = 0.1;
        //ctx1.filter = 'blur(2px)';
*/
    main();
    };
  })
        .catch(function(err) {
          console.log(err.name + ": " + err.message);
        });

      }


// Each face object contains a `scaledMesh` property,
// which is an array of 468 landmarks.

//faces.forEach(face => console.log(face.scaledMesh));



async function main()
{
	  //stats.showPanel(0);  // 0: fps, 1: ms, 2: mb, 3+: custom
     // document.getElementById('main').appendChild(stats.dom);
        // Load the MediaPipe facemesh model assets.
        const model = await facemesh.load({maxFaces:1});
        




function head_pose_estimation(faces,rightEye,leftEye)
{
  let fecePoints = tf.tensor(faces);
  let eye1 = tf.tensor1d(rightEye);
  let eye2 = tf.tensor1d(leftEye);

  let scales = fecePoints.div(tf.norm(eye1.sub(eye2))).mul(0.06);
  let centered = scales.sub(scales.mean(axis=0));

  let c00 = centered.slice(0,1).as1D();
  let c09 = centered.slice(9,1).as1D();
  let c18 = centered.slice(18,1).as1D();
  let c27 = centered.slice(27,1).as1D();

  let rotate0 = c18.sub(c00).div(tf.norm(c18.sub(c00)));
  let rotate1 = c09.sub(c27).div(tf.norm(c09.sub(c27)));

  let rotate = tf.concat([rotate0, rotate1]).arraySync();

  // rotation_matrix
  let m00 = rotate[0];
  let m01 = rotate[1];
  let m02 = rotate[2];

  let m10 = rotate[3];
  let m11 = rotate[4];
  let m12 = rotate[5];

  // cross product
  let m20 = m01*m12 - m02*m11;
  let m21 = m02*m10 - m00*m12;
  let m22 = m00*m11 - m01*m10;

  let sy = Math.sqrt(m00 * m00 +  m10 * m10);
  let singular = sy < 10**-6;

  if (!singular){
    yaw = Math.atan2(m21 , m22);
    pitch = Math.atan2(-m20, sy);
    roll = Math.atan2(m10, m00);
  } else {
    yaw = Math.atan2(-m12, m11);
    pitch = Math.atan2(-m20, sy);
    roll = 0;
  }

  const head = currentVRM.humanoid.getBoneNode( THREE.VRMSchema.HumanoidBoneName.Head );
  head.rotation.set( yaw + Math.PI , pitch, -(roll - Math.PI / 2)  , 'XYZ' );
  renderer.render( scene, camera );
  
}








        // Pass in a video stream to the model to obtain
        // a face from the MediaPipe graph.
        const video = document.querySelector("video");
        // let hands = await model.estimateHands(video);
       
/*
         scatterGL = new ScatterGL(
        document.querySelector('#scatter-gl-container'),
        {'rotateOnStart': false, 'selectEnabled': false});
*/
        // Each face object contains a `landmarks` property,
        // which is an array of 21 3-D landmarks.
 async function tracking()
 {
 	      stats.begin();
          let faces = await model.estimateFaces(video);

//↓この位置超重要！！
    /*   ctx1.clearRect(0, 0, canvas.width, canvas.height); */


let faceEntity = document.querySelector('#face');
let spList = document.querySelectorAll('.sp');
spList.forEach(sp => sp.parentNode.removeChild(sp));


  if (faces.length > 0)
  {
    for (let i = 0; i < faces.length; i++)
    {
      const keypoints = faces[i].scaledMesh;

/*
for (let i = 0; i < keypoints.length; i++) 
       {
*/
          let sp = document.createElement("a-plane");
          let keypointX = keypoints[8][0];
          let keypointY = keypoints[8][1];
          let keypointZ = keypoints[8][2] * 2.5;
          sp.setAttribute("position", `${keypointX} ${keypointY} ${keypointZ}`);
          sp.className = "sp";
          //sp.setAttribute("color", "red");
          sp.setAttribute("src", "#img");
          sp.setAttribute("scale", "-1 1 1");
          sp.setAttribute("rotation", "0 180 180");
          sp.setAttribute("width", "100");
          sp.setAttribute("height", "100");
          faceEntity.appendChild(sp);



    }



    if (scatterGL != null)
    {
      const pointsData = faces.map(face =>
      {
        let scaledMesh = face.scaledMesh;
        return scaledMesh.map(point => ([-point[0], -point[1], -point[2]]));
      });

      let flattenedPointsData = [];
      for (let i = 0; i < pointsData.length; i++) {
        flattenedPointsData = flattenedPointsData.concat(pointsData[i]);
      }
      const dataset = new ScatterGL.Dataset(flattenedPointsData);

      if (!scatterGLHasInitialized) {
        scatterGL.render(dataset);
      } else {
        scatterGL.updateDataset(dataset);
      }
      scatterGLHasInitialized = true;
    }




  }


 stats.end();
 requestAnimationFrame(tracking);
 };
 tracking();
}



</script>
</html>

